{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2148b9f4-c98b-44c3-9ab9-359e52730359",
   "metadata": {},
   "source": [
    "## PySpark Basics in CML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f53dde-24d0-42d8-a4fa-9ef056d47a11",
   "metadata": {},
   "source": [
    "**Apache Spark is a general purpose framework for distributed computing that offers high performance for both batch and stream processing. It exposes APIs for Java, Python, R, and Scala, as well as an interactive shell for you to run jobs.**\n",
    "\n",
    "**In Cloudera Machine Learning (CML), Spark and its dependencies are bundled directly into the CML engine Docker image or CML Runtime. CML supports fully-containerized execution of Spark workloads via Spark's support for the Kubernetes cluster backend. Users can interact with Spark both interactively and in batch mode.**\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a8b8a0-21a3-4f8a-beaf-bf8bda467671",
   "metadata": {
    "tags": []
   },
   "source": [
    "![title](img/sparkonk8s.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26860dbb-87fe-442a-aff4-82e9879394f8",
   "metadata": {},
   "source": [
    "**The bottom line: Spark on CML is dramatically easier to use than Spark on Yarn on CDSW or other platforms. There is no need to distribute dependencies among execuotors. You can easily spin up multi-executor distributed Spark Sessions and interrupt them as needed, all within quotas and other cost monitoring constraints**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26037ffe-f8c9-49e6-979c-91b3564a91ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3758592-d2bc-4ce2-a2b9-2d155ff9a910",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97b0d0e-ec59-4e7d-b010-79d7ae6c43c7",
   "metadata": {},
   "source": [
    "**A simple, single pod Spark Session can be instantiated as shown below. This creates a single pod for the Spark application**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce7cb2e7-47d7-4d68-8782-09356b9843bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/16 00:37:38 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:135)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/03/16 00:37:38 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:135)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/03/16 00:37:38 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .appName(\"SimpleSession\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "192cf257-2b34-4d8f-b69a-0004d1d599c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://100.100.8.239:20049\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.3.1.19.7215.0-118</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SimpleSession</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe84c8e8040>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2072f567-f206-442c-b570-f2586a4a7249",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://spark-ikgvjqk3qptnit2c.ml-d7f9c760-9de.go01-dem.ylcu-atmi.cloudera.site\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"https://spark-\"+os.environ[\"CDSW_ENGINE_ID\"]+\".\"+os.environ[\"CDSW_DOMAIN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0fee31-2c9c-4b37-9eff-ede644f5e079",
   "metadata": {},
   "source": [
    "You can reach the Spark UI from the URL as shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f8f2ab-f3e7-4349-a867-bf7bfb202092",
   "metadata": {},
   "source": [
    "![title](img/spark_ui.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db12df6-ce12-4430-82bd-8fb297c11cd6",
   "metadata": {},
   "source": [
    "You can stop the Spark session any time as below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d902e21-2c74-443a-afe8-0c8fb7045d4f",
   "metadata": {},
   "source": [
    "Let's try a Spark Session with more horsepower. You can increase number of cores and memory for the Driver and Executors with the following settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab340e8-3bc7-43b1-9936-72b147e863c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"DistributedSession\")\\\n",
    "    .config(\"spark.executor.memory\",\"2g\")\\\n",
    "    .config(\"spark.executor.cores\",\"5\")\\\n",
    "    .config(\"spark.driver.memory\",\"2g\")\\\n",
    "    .getOrCreate()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a1eb4e-f8f5-4813-82f1-478d5bb74cc0",
   "metadata": {},
   "source": [
    "You just created a distributed Spark Session with 1 Driver and 5 Executors. Each Executor has 8 cores and 2GB of memory. Not bad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "523db608-adcc-43f7-874f-854a8a2e8e9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82c4215-62c1-4d73-9845-352a7f907867",
   "metadata": {},
   "source": [
    "## Spark Data Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aab8d0-1921-4d32-947e-5074757b4ac0",
   "metadata": {},
   "source": [
    "The best way to launch a spark session is however Spark Data Connections. This feature provides the code you need to launch your Spark Session and already includes the Iceberg jars to enable Lakehouse analytics!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1f92ac-be4b-4a8a-98e5-f58b7de9bb63",
   "metadata": {},
   "source": [
    "To launch a Spark Data Connection, open \"Data\" and pick the \"Spark\" option. Then copy paste the code into the cell below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e68c3b6-548d-4ce6-8a8c-49e774828feb",
   "metadata": {
    "tags": []
   },
   "source": [
    "![title](img/dataconnections.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fea1dce7-f895-46c4-abc1-3191dd24580e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|         01_car_data|\n",
      "|           01_car_dw|\n",
      "|              adb101|\n",
      "|            airlines|\n",
      "|        airlines_csv|\n",
      "|    airlines_iceberg|\n",
      "|airlines_iceberg_...|\n",
      "|      airlines_maint|\n",
      "|      airlines_mjain|\n",
      "|          airquality|\n",
      "|                ajvp|\n",
      "|     akahan_airlines|\n",
      "|           amallegni|\n",
      "|          atlas_demo|\n",
      "|            bankdemo|\n",
      "|          bca_jps_l0|\n",
      "|      bnk_mlops_demo|\n",
      "|       bnk_mlops_hol|\n",
      "|     bri_ranger_demo|\n",
      "|            cdc_data|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cml.data_v1 as cmldata\n",
    "\n",
    "# Sample in-code customization of spark configurations\n",
    "#from pyspark import SparkContext\n",
    "#SparkContext.setSystemProperty('spark.executor.cores', '1')\n",
    "#SparkContext.setSystemProperty('spark.executor.memory', '2g')\n",
    "\n",
    "CONNECTION_NAME = \"go01-aw-dl\"\n",
    "conn = cmldata.get_connection(CONNECTION_NAME)\n",
    "spark = conn.get_spark_session()\n",
    "\n",
    "# Sample usage to run query through spark\n",
    "EXAMPLE_SQL_QUERY = \"show databases\"\n",
    "spark.sql(EXAMPLE_SQL_QUERY).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-capital",
   "metadata": {},
   "source": [
    "## Intro to Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "mineral-likelihood",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "increasing-blackjack",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"inferschema\", True).option(\"header\", True).csv('data/LoanStats_2015_subset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "wireless-acrobat",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe Shape\n",
      "(29796, 105)\n"
     ]
    }
   ],
   "source": [
    "#Printing number of rows and columns:\n",
    "print('Dataframe Shape')\n",
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10880ecc-2f98-4afb-bb15-2ecb78ba0866",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-peoples",
   "metadata": {},
   "source": [
    "## KPI Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e6b79a9-45de-4d2c-9baa-6984885bd401",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import seaborn as sns\n",
    "from pyspark.ml.feature import Bucketizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab02a5ff-5e82-431b-9553-add0317a924d",
   "metadata": {},
   "source": [
    "#### Min and Max Loan Amount "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7612618-17d0-4ca4-987f-6471588f8785",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loan Amount\n",
      "+--------------+\n",
      "|min(loan_amnt)|\n",
      "+--------------+\n",
      "|          1000|\n",
      "+--------------+\n",
      "\n",
      "Maximum Loan Amount\n",
      "+--------------+\n",
      "|max(loan_amnt)|\n",
      "+--------------+\n",
      "|         35000|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Minimum Loan Amount\")\n",
    "df.agg(F.min(\"loan_amnt\")).show()\n",
    "\n",
    "print(\"Maximum Loan Amount\")\n",
    "df.agg(F.max(\"loan_amnt\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6526352f-94bd-484b-a1ab-c30e043c5839",
   "metadata": {},
   "source": [
    "#### Create Bins from the Loan Amount Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8d245df-9c77-4bf3-af86-43eb817cd1dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2 = Bucketizer(\n",
    "    splits=[-float('inf'), 10000, 20000, float('inf')],\n",
    "    inputCol='loan_amnt',\n",
    "    outputCol='loan_amnt_bin'\n",
    ").transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8c8a1fc-d044-402c-87ec-8d6346c8f401",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29791</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29792</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29793</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29794</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29795</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29796 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       loan_amnt_bin\n",
       "0                1.0\n",
       "1                2.0\n",
       "2                2.0\n",
       "3                2.0\n",
       "4                1.0\n",
       "...              ...\n",
       "29791            2.0\n",
       "29792            2.0\n",
       "29793            2.0\n",
       "29794            1.0\n",
       "29795            1.0\n",
       "\n",
       "[29796 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_pandas = df2.select(\"loan_amnt_bin\").toPandas()\n",
    "df2_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d967d331-4295-4841-b43e-778925879b22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x=\"loan_amnt_bin\", data=df2_pandas).set(title='Count By Loan Amount Categories')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4d41b0-a0b3-4708-b835-707c62654143",
   "metadata": {},
   "source": [
    "### Rounded Mean Funded Amount By Loan Amount Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "rough-keyboard",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------------+\n",
      "|loan_amnt_bin|CEIL(avg(funded_amnt))|\n",
      "+-------------+----------------------+\n",
      "|          0.0|                  5823|\n",
      "|          1.0|                 13751|\n",
      "|          2.0|                 26144|\n",
      "+-------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#how many loans defaulted for each month (all data is 2015):\n",
    "df2.groupby('loan_amnt_bin').mean('funded_amnt').na.drop().select(\"loan_amnt_bin\", F.ceil(F.col(\"avg(funded_amnt)\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-parcel",
   "metadata": {},
   "source": [
    "### Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "surprising-communications",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|         01_car_data|\n",
      "|           01_car_dw|\n",
      "|              adb101|\n",
      "|            airlines|\n",
      "|        airlines_csv|\n",
      "|    airlines_iceberg|\n",
      "|airlines_iceberg_...|\n",
      "|      airlines_maint|\n",
      "|      airlines_mjain|\n",
      "|          airquality|\n",
      "|                ajvp|\n",
      "|     akahan_airlines|\n",
      "|           amallegni|\n",
      "|          atlas_demo|\n",
      "|            bankdemo|\n",
      "|          bca_jps_l0|\n",
      "|      bnk_mlops_demo|\n",
      "|       bnk_mlops_hol|\n",
      "|     bri_ranger_demo|\n",
      "|            cdc_data|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "competent-myanmar",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------+\n",
      "|namespace|           tableName|isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "|  default|gartner_florida_s...|      false|\n",
      "|  default|                   t|      false|\n",
      "|  default|                   s|      false|\n",
      "|  default|        driver_stats|      false|\n",
      "|  default|      shaun_ice_ty21|      false|\n",
      "|  default|      hue__tmp_test2|      false|\n",
      "|  default|        test_varchar|      false|\n",
      "|  default|  churn_prototype_fh|      false|\n",
      "|  default|churn_prototype_t...|      false|\n",
      "|  default|           t2_impala|      false|\n",
      "|  default|  test_varchar_parq2|      false|\n",
      "|  default|      loan_prototype|      false|\n",
      "|  default|        tax_auth_heb|      false|\n",
      "|  default|                test|      false|\n",
      "|  default|     results_anomaly|      false|\n",
      "|  default|   churn_mengelhardt|      false|\n",
      "|  default|             mytable|      false|\n",
      "|  default|        transactions|      false|\n",
      "|  default|               ice_t|      false|\n",
      "|  default|           customers|      false|\n",
      "+---------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b34297-b042-46bb-984e-dcdf4727b047",
   "metadata": {},
   "source": [
    "#### Saving the Table As Temporary View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2fe1e885-bdb6-4173-b290-b17042f74c17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/16 00:47:54 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "## Registering the dataframe as a temporary table:\n",
    "## Temporary views in Spark SQL are session-scoped and will disappear if the session that creates it terminates. \n",
    "\n",
    "df2.createOrReplaceTempView(\"my_temp_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4edf124-12f0-4968-930f-c3c6d3e1519e",
   "metadata": {},
   "source": [
    "#### Saving the Table Permanently in CDW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e49e5525-363f-497e-a440-86ec413aa683",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/16 00:48:20 WARN ShellBasedUnixGroupsMapping: unable to return groups for user pauldefusco\n",
      "PartialGroupNameException The user name 'pauldefusco' is not found. id: ‘pauldefusco’: no such user\n",
      "id: ‘pauldefusco’: no such user\n",
      "\n",
      "\tat org.apache.hadoop.security.ShellBasedUnixGroupsMapping.resolvePartialGroupNames(ShellBasedUnixGroupsMapping.java:291)\n",
      "\tat org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:215)\n",
      "\tat org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroupsSet(ShellBasedUnixGroupsMapping.java:123)\n",
      "\tat org.apache.hadoop.security.Groups$GroupCacheLoader.fetchGroupSet(Groups.java:413)\n",
      "\tat org.apache.hadoop.security.Groups$GroupCacheLoader.load(Groups.java:351)\n",
      "\tat org.apache.hadoop.security.Groups$GroupCacheLoader.load(Groups.java:300)\n",
      "\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)\n",
      "\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)\n",
      "\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)\n",
      "\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)\n",
      "\tat com.google.common.cache.LocalCache.get(LocalCache.java:3953)\n",
      "\tat com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3976)\n",
      "\tat com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4960)\n",
      "\tat org.apache.hadoop.security.Groups.getGroupInternal(Groups.java:258)\n",
      "\tat org.apache.hadoop.security.Groups.getGroupsSet(Groups.java:230)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.getGroupsSet(UserGroupInformation.java:1761)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1727)\n",
      "\tat org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator.setConf(HadoopDefaultAuthenticator.java:64)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:77)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:137)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.HiveUtils.getAuthenticator(HiveUtils.java:408)\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.setupAuth(SessionState.java:1004)\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.getAuthenticator(SessionState.java:1788)\n",
      "\tat org.apache.hadoop.hive.ql.session.SessionState.getUserFromAuthenticator(SessionState.java:1388)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Table.getEmptyTable(Table.java:230)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Table.<init>(Table.java:157)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl$.toHiveTable(HiveClientImpl.scala:1138)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$createTable$1(HiveClientImpl.scala:607)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:331)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:258)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:257)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:311)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:605)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:510)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createDataSourceTable(HiveExternalCatalog.scala:399)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createTable$1(HiveExternalCatalog.scala:274)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:245)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:100)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:380)\n",
      "\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:191)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:697)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:675)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:565)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "## The table will now be a Hive External Table and will show in CDW and Atlas.\n",
    "## Ranger Policies can now be set on the Table\n",
    "\n",
    "df2.select([\"funded_amnt\", \"loan_amnt\", \"installment\", \"tot_cur_bal\"]).write.format('parquet').mode(\"overwrite\").saveAsTable('default.my_hive_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "795ab942-0457-44c0-9077-cc0c3c99e442",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddb9b78-ca1c-48f9-a4ba-b1f6b2bed47b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
