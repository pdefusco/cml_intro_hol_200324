{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b633a7f-947a-460f-948a-e0a377fa3149",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#****************************************************************************\n",
    "# (C) Cloudera, Inc. 2020-2024\n",
    "#  All rights reserved.\n",
    "#\n",
    "#  Applicable Open Source License: GNU Affero General Public License v3.0\n",
    "#\n",
    "#  NOTE: Cloudera open source products are modular software products\n",
    "#  made up of hundreds of individual components, each of which was\n",
    "#  individually copyrighted.  Each Cloudera open source product is a\n",
    "#  collective work under U.S. Copyright Law. Your license to use the\n",
    "#  collective work is as provided in your written agreement with\n",
    "#  Cloudera.  Used apart from the collective work, this file is\n",
    "#  licensed for your use pursuant to the open source license\n",
    "#  identified above.\n",
    "#\n",
    "#  This code is provided to you pursuant a written agreement with\n",
    "#  (i) Cloudera, Inc. or (ii) a third-party authorized to distribute\n",
    "#  this code. If you do not have a written agreement with Cloudera nor\n",
    "#  with an authorized and properly licensed third party, you do not\n",
    "#  have any rights to access nor to use this code.\n",
    "#\n",
    "#  Absent a written agreement with Cloudera, Inc. (“Cloudera”) to the\n",
    "#  contrary, A) CLOUDERA PROVIDES THIS CODE TO YOU WITHOUT WARRANTIES OF ANY\n",
    "#  KIND; (B) CLOUDERA DISCLAIMS ANY AND ALL EXPRESS AND IMPLIED\n",
    "#  WARRANTIES WITH RESPECT TO THIS CODE, INCLUDING BUT NOT LIMITED TO\n",
    "#  IMPLIED WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY AND\n",
    "#  FITNESS FOR A PARTICULAR PURPOSE; (C) CLOUDERA IS NOT LIABLE TO YOU,\n",
    "#  AND WILL NOT DEFEND, INDEMNIFY, NOR HOLD YOU HARMLESS FOR ANY CLAIMS\n",
    "#  ARISING FROM OR RELATED TO THE CODE; AND (D)WITH RESPECT TO YOUR EXERCISE\n",
    "#  OF ANY RIGHTS GRANTED TO YOU FOR THE CODE, CLOUDERA IS NOT LIABLE FOR ANY\n",
    "#  DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, PUNITIVE OR\n",
    "#  CONSEQUENTIAL DAMAGES INCLUDING, BUT NOT LIMITED TO, DAMAGES\n",
    "#  RELATED TO LOST REVENUE, LOST PROFITS, LOSS OF INCOME, LOSS OF\n",
    "#  BUSINESS ADVANTAGE OR UNAVAILABILITY, OR LOSS OR CORRUPTION OF\n",
    "#  DATA.\n",
    "#\n",
    "# #  Author(s): Paul de Fusco\n",
    "#***************************************************************************/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "836ce1c9-ab2e-4497-a5a0-006e710b7c05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping SedonaKepler import, verify if keplergl is installed\n",
      "Skipping SedonaPyDeck import, verify if pydeck is installed\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from sedona.register import SedonaRegistrator\n",
    "import os, warnings, sys, logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import cml.data_v1 as cmldata\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel\n",
    "import pandas as pd\n",
    "from sedona.spark import *\n",
    "from sedona.core.geom.envelope import Envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa8f3225-ae0f-41ec-b173-ac2bde78c873",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "USERNAME = os.environ[\"PROJECT_OWNER\"]\n",
    "DBNAME = \"LOGISTICS_MLOPS_DEMO\"\n",
    "STORAGE = \"s3a://goes-se-sandbox01\"\n",
    "CONNECTION_NAME = \"se-aw-mdl\"\n",
    "DATE = date.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8235eb92-6538-44ad-9fb1-95177b391c1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://artifacts.unidata.ucar.edu/repository/unidata-all added as a remote repository with the name: repo-1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/runtime-addons/spark323-19-hf2-e51pfs/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/cdsw/.ivy2/cache\n",
      "The jars for the packages stored in: /home/cdsw/.ivy2/jars\n",
      "org.apache.sedona#sedona-spark-3.0_2.12 added as a dependency\n",
      "org.datasyslab#geotools-wrapper added as a dependency\n",
      "uk.co.gresearch.spark#spark-extension_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-52489182-d476-45c3-9e19-cd12e6ed3e8d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.sedona#sedona-spark-3.0_2.12;1.5.1 in central\n",
      "\tfound org.apache.sedona#sedona-common;1.5.1 in central\n",
      "\tfound org.apache.commons#commons-math3;3.6.1 in central\n",
      "\tfound org.locationtech.jts#jts-core;1.19.0 in central\n",
      "\tfound org.wololo#jts2geojson;0.16.1 in central\n",
      "\tfound org.locationtech.spatial4j#spatial4j;0.8 in central\n",
      "\tfound com.google.geometry#s2-geometry;2.0.0 in central\n",
      "\tfound com.google.guava#guava;25.1-jre in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound org.checkerframework#checker-qual;2.0.0 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.1.3 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.1 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.14 in central\n",
      "\tfound com.uber#h3;4.1.1 in central\n",
      "\tfound net.sf.geographiclib#GeographicLib-Java;1.52 in central\n",
      "\tfound com.github.ben-manes.caffeine#caffeine;2.9.2 in central\n",
      "\tfound org.checkerframework#checker-qual;3.10.0 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.5.1 in central\n",
      "\tfound org.apache.sedona#sedona-spark-common-3.0_2.12;1.5.1 in central\n",
      "\tfound commons-lang#commons-lang;2.6 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.5.0 in central\n",
      "\tfound org.beryx#awt-color-factory;1.0.0 in central\n",
      "\tfound org.datasyslab#geotools-wrapper;1.5.1-28.2 in central\n",
      "\tfound uk.co.gresearch.spark#spark-extension_2.12;2.11.0-3.4 in central\n",
      "\tfound com.github.scopt#scopt_2.12;4.1.0 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/sedona/sedona-spark-3.0_2.12/1.5.1/sedona-spark-3.0_2.12-1.5.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.sedona#sedona-spark-3.0_2.12;1.5.1!sedona-spark-3.0_2.12.jar (90ms)\n",
      "downloading https://repo1.maven.org/maven2/org/datasyslab/geotools-wrapper/1.5.1-28.2/geotools-wrapper-1.5.1-28.2.jar ...\n",
      "\t[SUCCESSFUL ] org.datasyslab#geotools-wrapper;1.5.1-28.2!geotools-wrapper.jar (551ms)\n",
      "downloading https://repo1.maven.org/maven2/uk/co/gresearch/spark/spark-extension_2.12/2.11.0-3.4/spark-extension_2.12-2.11.0-3.4.jar ...\n",
      "\t[SUCCESSFUL ] uk.co.gresearch.spark#spark-extension_2.12;2.11.0-3.4!spark-extension_2.12.jar (62ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/sedona/sedona-common/1.5.1/sedona-common-1.5.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.sedona#sedona-common;1.5.1!sedona-common.jar (72ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/sedona/sedona-spark-common-3.0_2.12/1.5.1/sedona-spark-common-3.0_2.12-1.5.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.sedona#sedona-spark-common-3.0_2.12;1.5.1!sedona-spark-common-3.0_2.12.jar (109ms)\n",
      "downloading https://repo1.maven.org/maven2/org/locationtech/jts/jts-core/1.19.0/jts-core-1.19.0.jar ...\n",
      "\t[SUCCESSFUL ] org.locationtech.jts#jts-core;1.19.0!jts-core.jar(bundle) (81ms)\n",
      "downloading https://repo1.maven.org/maven2/org/wololo/jts2geojson/0.16.1/jts2geojson-0.16.1.jar ...\n",
      "\t[SUCCESSFUL ] org.wololo#jts2geojson;0.16.1!jts2geojson.jar (44ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-collection-compat_2.12/2.5.0/scala-collection-compat_2.12-2.5.0.jar ...\n",
      "\t[SUCCESSFUL ] org.scala-lang.modules#scala-collection-compat_2.12;2.5.0!scala-collection-compat_2.12.jar (64ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-math3/3.6.1/commons-math3-3.6.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-math3;3.6.1!commons-math3.jar (104ms)\n",
      "downloading https://repo1.maven.org/maven2/org/locationtech/spatial4j/spatial4j/0.8/spatial4j-0.8.jar ...\n",
      "\t[SUCCESSFUL ] org.locationtech.spatial4j#spatial4j;0.8!spatial4j.jar(bundle) (56ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/geometry/s2-geometry/2.0.0/s2-geometry-2.0.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.geometry#s2-geometry;2.0.0!s2-geometry.jar (68ms)\n",
      "downloading https://repo1.maven.org/maven2/com/uber/h3/4.1.1/h3-4.1.1.jar ...\n",
      "\t[SUCCESSFUL ] com.uber#h3;4.1.1!h3.jar (93ms)\n",
      "downloading https://repo1.maven.org/maven2/net/sf/geographiclib/GeographicLib-Java/1.52/GeographicLib-Java-1.52.jar ...\n",
      "\t[SUCCESSFUL ] net.sf.geographiclib#GeographicLib-Java;1.52!GeographicLib-Java.jar (41ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/ben-manes/caffeine/caffeine/2.9.2/caffeine-2.9.2.jar ...\n",
      "\t[SUCCESSFUL ] com.github.ben-manes.caffeine#caffeine;2.9.2!caffeine.jar (76ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/guava/guava/25.1-jre/guava-25.1-jre.jar ...\n",
      "\t[SUCCESSFUL ] com.google.guava#guava;25.1-jre!guava.jar(bundle) (103ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.2!jsr305.jar (43ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/j2objc/j2objc-annotations/1.1/j2objc-annotations-1.1.jar ...\n",
      "\t[SUCCESSFUL ] com.google.j2objc#j2objc-annotations;1.1!j2objc-annotations.jar (43ms)\n",
      "downloading https://repo1.maven.org/maven2/org/codehaus/mojo/animal-sniffer-annotations/1.14/animal-sniffer-annotations-1.14.jar ...\n",
      "\t[SUCCESSFUL ] org.codehaus.mojo#animal-sniffer-annotations;1.14!animal-sniffer-annotations.jar (39ms)\n",
      "downloading https://repo1.maven.org/maven2/org/checkerframework/checker-qual/3.10.0/checker-qual-3.10.0.jar ...\n",
      "\t[SUCCESSFUL ] org.checkerframework#checker-qual;3.10.0!checker-qual.jar (66ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/errorprone/error_prone_annotations/2.5.1/error_prone_annotations-2.5.1.jar ...\n",
      "\t[SUCCESSFUL ] com.google.errorprone#error_prone_annotations;2.5.1!error_prone_annotations.jar (43ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-lang/commons-lang/2.6/commons-lang-2.6.jar ...\n",
      "\t[SUCCESSFUL ] commons-lang#commons-lang;2.6!commons-lang.jar (66ms)\n",
      "downloading https://repo1.maven.org/maven2/org/beryx/awt-color-factory/1.0.0/awt-color-factory-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.beryx#awt-color-factory;1.0.0!awt-color-factory.jar (46ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/scopt/scopt_2.12/4.1.0/scopt_2.12-4.1.0.jar ...\n",
      "\t[SUCCESSFUL ] com.github.scopt#scopt_2.12;4.1.0!scopt_2.12.jar (72ms)\n",
      ":: resolution report :: resolve 12590ms :: artifacts dl 2134ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.ben-manes.caffeine#caffeine;2.9.2 from central in [default]\n",
      "\tcom.github.scopt#scopt_2.12;4.1.0 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.5.1 from central in [default]\n",
      "\tcom.google.geometry#s2-geometry;2.0.0 from central in [default]\n",
      "\tcom.google.guava#guava;25.1-jre from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.1 from central in [default]\n",
      "\tcom.uber#h3;4.1.1 from central in [default]\n",
      "\tcommons-lang#commons-lang;2.6 from central in [default]\n",
      "\tnet.sf.geographiclib#GeographicLib-Java;1.52 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.6.1 from central in [default]\n",
      "\torg.apache.sedona#sedona-common;1.5.1 from central in [default]\n",
      "\torg.apache.sedona#sedona-spark-3.0_2.12;1.5.1 from central in [default]\n",
      "\torg.apache.sedona#sedona-spark-common-3.0_2.12;1.5.1 from central in [default]\n",
      "\torg.beryx#awt-color-factory;1.0.0 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.10.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.14 from central in [default]\n",
      "\torg.datasyslab#geotools-wrapper;1.5.1-28.2 from central in [default]\n",
      "\torg.locationtech.jts#jts-core;1.19.0 from central in [default]\n",
      "\torg.locationtech.spatial4j#spatial4j;0.8 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.5.0 from central in [default]\n",
      "\torg.wololo#jts2geojson;0.16.1 from central in [default]\n",
      "\tuk.co.gresearch.spark#spark-extension_2.12;2.11.0-3.4 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.checkerframework#checker-qual;2.0.0 by [org.checkerframework#checker-qual;3.10.0] in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.1.3 by [com.google.errorprone#error_prone_annotations;2.5.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   25  |   25  |   25  |   2   ||   23  |   23  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-52489182-d476-45c3-9e19-cd12e6ed3e8d\n",
      "\tconfs: [default]\n",
      "\t23 artifacts copied, 0 already retrieved (42536kB/1150ms)\n",
      "Setting spark.hadoop.yarn.resourcemanager.principal to pauldefusco\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/16 01:39:42 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:135)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/03/16 01:39:42 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:135)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/03/16 01:39:43 WARN HiveServer2CredentialProvider: Failed to get HS2 delegation token\n",
      "java.util.NoSuchElementException: spark.sql.hive.hiveserver2.jdbc.url\n",
      "\tat org.apache.spark.SparkConf.$anonfun$get$1(SparkConf.scala:245)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.SparkConf.get(SparkConf.scala:245)\n",
      "\tat com.hortonworks.spark.deploy.yarn.security.HiveServer2CredentialProvider.obtainDelegationTokens(HiveServer2CredentialProvider.scala:64)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.$anonfun$obtainDelegationTokens$2(HadoopDelegationTokenManager.scala:164)\n",
      "\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.MapLike$DefaultValuesIterable.foreach(MapLike.scala:213)\n",
      "\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)\n",
      "\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)\n",
      "\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.org$apache$spark$deploy$security$HadoopDelegationTokenManager$$obtainDelegationTokens(HadoopDelegationTokenManager.scala:162)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager$$anon$4.run(HadoopDelegationTokenManager.scala:226)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager$$anon$4.run(HadoopDelegationTokenManager.scala:224)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.obtainTokensAndScheduleRenewal(HadoopDelegationTokenManager.scala:224)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.org$apache$spark$deploy$security$HadoopDelegationTokenManager$$updateTokensTask(HadoopDelegationTokenManager.scala:198)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.start(HadoopDelegationTokenManager.scala:123)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.$anonfun$start$1(CoarseGrainedSchedulerBackend.scala:552)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.$anonfun$start$1$adapted(CoarseGrainedSchedulerBackend.scala:549)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.start(CoarseGrainedSchedulerBackend.scala:549)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.KubernetesClusterSchedulerBackend.start(KubernetesClusterSchedulerBackend.scala:95)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:220)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:581)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/03/16 01:39:43 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/03/16 01:39:43 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/03/16 01:39:44 WARN ExecutorAllocationManager: Dynamic allocation without a shuffle service is an experimental feature.\n",
      "24/03/16 01:39:44 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    }
   ],
   "source": [
    "config = SedonaContext.builder() .\\\n",
    "    config('spark.jars.packages',\n",
    "           'org.apache.sedona:sedona-spark-3.0_2.12:1.5.1,'\n",
    "           'org.datasyslab:geotools-wrapper:1.5.1-28.2,'\n",
    "           'uk.co.gresearch.spark:spark-extension_2.12:2.11.0-3.4'). \\\n",
    "    config('spark.jars.repositories', 'https://artifacts.unidata.ucar.edu/repository/unidata-all'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8046970a-feaa-4a4c-9f57-d9ff0488cf38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sedona = SedonaContext.create(config)\n",
    "sc = sedona.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9de2b10-783a-4ef3-89d0-1cee376599af",
   "metadata": {},
   "source": [
    "### Import State Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5044f1d-8df5-4660-b636-9b485fbbe6c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "states_wkt = sedona.read.option(\"delimiter\", \"\\t\")\\\n",
    "                    .option(\"header\", \"false\")\\\n",
    "                    .csv(\"data/boundary-each-state.tsv\")\\\n",
    "                    .toDF(\"s_name\",\"s_bound\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c444afa1-5e51-4d60-b2bb-695d3c865444",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|       s_name|             s_bound|\n",
      "+-------------+--------------------+\n",
      "|       Alaska|POLYGON ((-141.02...|\n",
      "|      Alabama|POLYGON ((-88.195...|\n",
      "|     Arkansas|POLYGON ((-94.041...|\n",
      "|      Arizona|POLYGON ((-112.59...|\n",
      "|   California|POLYGON ((-124.40...|\n",
      "|     Colorado|POLYGON ((-109.04...|\n",
      "|  Connecticut|POLYGON ((-73.487...|\n",
      "|     Delaware|POLYGON ((-75.791...|\n",
      "|      Florida|POLYGON ((-87.605...|\n",
      "|      Georgia|POLYGON ((-85.608...|\n",
      "|       Hawaii|POLYGON ((-154.62...|\n",
      "|         Iowa|POLYGON ((-95.762...|\n",
      "|        Idaho|POLYGON ((-117.03...|\n",
      "|     Illinois|POLYGON ((-90.629...|\n",
      "|      Indiana|POLYGON ((-87.525...|\n",
      "|       Kansas|POLYGON ((-102.05...|\n",
      "|     Kentucky|POLYGON ((-89.537...|\n",
      "|    Louisiana|POLYGON ((-94.043...|\n",
      "|Massachusetts|POLYGON ((-72.778...|\n",
      "|     Maryland|POLYGON ((-79.477...|\n",
      "+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- s_name: string (nullable = true)\n",
      " |-- s_bound: geometry (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "states = states_wkt.selectExpr(\"s_name\", \"ST_GeomFromWKT(s_bound) as s_bound\")\n",
    "states.show()\n",
    "states.printSchema()\n",
    "states.createOrReplaceTempView(\"states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e33d11e-cff6-4ce0-9f8a-3af6edc50c98",
   "metadata": {},
   "source": [
    "### Import IOT Fleet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a256a6fd-99bb-432a-8a1a-4aaec49d5a29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iotDf = sedona.read.option(\"delimiter\", \",\")\\\n",
    "            .option(\"header\", \"true\")\\\n",
    "            .csv(\"data/iot_fleet_data.csv\")\n",
    "\n",
    "iotDf.createOrReplaceTempView(\"IOT_FLEET_DATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d1f3607-0b03-48b2-ba88-49e86bcd8eed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+------------------+\n",
      "|          iot_coords|      device_id|        event_type|\n",
      "+--------------------+---------------+------------------+\n",
      "|POINT (42.297974 ...|0x1000000000005|system malfunction|\n",
      "|POINT (41.75997 -...|0x100000000001d|    tank below 10%|\n",
      "|POINT (41.908176 ...|0x1000000000008|    tank below 10%|\n",
      "|POINT (41.677345 ...|0x100000000001b|    tank below 10%|\n",
      "|POINT (42.031174 ...|0x1000000000014|    tank below 10%|\n",
      "|POINT (42.25368 -...|0x100000000001c|      device error|\n",
      "|POINT (42.423115 ...|0x1000000000005|system malfunction|\n",
      "|POINT (42.130592 ...|0x100000000000b|      device error|\n",
      "|POINT (42.339985 ...|0x1000000000024|    tank below 10%|\n",
      "|POINT (42.488728 ...|0x1000000000009|    tank below 10%|\n",
      "|POINT (42.01407 -...|0x1000000000025|      device error|\n",
      "|POINT (41.794743 ...|0x1000000000001|    tank below 10%|\n",
      "|POINT (41.784687 ...|0x1000000000020|system malfunction|\n",
      "|POINT (42.273937 ...|0x1000000000017|     tank below 5%|\n",
      "|POINT (41.66811 -...|0x1000000000022|    tank below 10%|\n",
      "|POINT (42.43808 -...|0x1000000000004|    tank below 10%|\n",
      "|POINT (42.206646 ...|0x100000000001e|      device error|\n",
      "|POINT (41.63239 -...|0x1000000000012|    tank below 10%|\n",
      "|POINT (41.753616 ...|0x100000000001e|     tank below 5%|\n",
      "|POINT (42.11225 -...|0x1000000000004|      device error|\n",
      "+--------------------+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iotGeoDf = sedona.sql(\"\"\"SELECT ST_Point(CAST(IOT_FLEET_DATA.latitude AS Decimal(24,20)),\n",
    "                CAST(IOT_FLEET_DATA.longitude AS Decimal(24,20))) AS iot_coords,\n",
    "                IOT_FLEET_DATA.device_id,\n",
    "                IOT_FLEET_DATA.event_type\n",
    "                FROM IOT_FLEET_DATA\n",
    "                \"\"\")\n",
    "\n",
    "iotGeoDf.show()\n",
    "\n",
    "iotGeoDf.createOrReplaceTempView(\"IOT_GEO_DATA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fc658c-3361-4f0c-aa24-8a8bce91a8c1",
   "metadata": {},
   "source": [
    "### What US State are the IOT Fleet Devices located in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cccbbc17-6875-436d-97c3-b182dc12d808",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+---------------+------------------+\n",
      "|s_name|             s_bound|          iot_coords|      device_id|        event_type|\n",
      "+------+--------------------+--------------------+---------------+------------------+\n",
      "|  Iowa|POLYGON ((-95.762...|POINT (41.59775 -...|0x1000000000013|    tank below 10%|\n",
      "|  Iowa|POLYGON ((-95.762...|POINT (41.609802 ...|0x100000000001f|     tank below 5%|\n",
      "|  Iowa|POLYGON ((-95.762...|POINT (41.630653 ...|0x100000000000a|system malfunction|\n",
      "|  Iowa|POLYGON ((-95.762...|POINT (41.63239 -...|0x1000000000012|    tank below 10%|\n",
      "|  Iowa|POLYGON ((-95.762...|POINT (41.633617 ...|0x100000000000a|     tank below 5%|\n",
      "|  Iowa|POLYGON ((-95.762...|POINT (41.63881 -...|0x1000000000003|      device error|\n",
      "|  Iowa|POLYGON ((-95.762...|POINT (41.64094 -...|0x1000000000027|      device error|\n",
      "|  Iowa|POLYGON ((-95.762...|POINT (41.64202 -...|0x1000000000008|     tank below 5%|\n",
      "|  Iowa|POLYGON ((-95.762...|POINT (41.6588 -9...|0x1000000000016|      device error|\n",
      "|  Iowa|POLYGON ((-95.762...|POINT (41.66011 -...|0x100000000000f|      device error|\n",
      "|  Iowa|POLYGON ((-95.762...|POINT (41.611935 ...|0x100000000000a|system malfunction|\n",
      "|  Iowa|POLYGON ((-95.762...|POINT (41.625294 ...|0x1000000000010|      device error|\n",
      "|  Iowa|POLYGON ((-95.762...|POINT (41.63686 -...|0x1000000000004|     tank below 5%|\n",
      "|  Iowa|POLYGON ((-95.762...|POINT (41.652485 ...|0x100000000001d|      device error|\n",
      "|  Iowa|POLYGON ((-95.762...|POINT (41.65364 -...|0x1000000000000|     tank below 5%|\n",
      "|  Iowa|POLYGON ((-95.762...|POINT (41.658474 ...|0x1000000000016|      device error|\n",
      "|  Iowa|POLYGON ((-95.762...|POINT (41.664223 ...|0x100000000001d|    tank below 10%|\n",
      "|  Iowa|POLYGON ((-95.762...|POINT (41.66449 -...|0x100000000001d|     tank below 5%|\n",
      "|  Iowa|POLYGON ((-95.762...|POINT (41.667557 ...|0x100000000000f|      device error|\n",
      "|  Iowa|POLYGON ((-95.762...|POINT (41.667904 ...|0x100000000001e|system malfunction|\n",
      "+------+--------------------+--------------------+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "containsDf = sedona.sql(\"\"\"SELECT * \n",
    "                            FROM states s, IOT_GEO_DATA iot \n",
    "                            WHERE ST_Covers(s.s_bound, ST_FlipCoordinates(iot.iot_coords))\"\"\")\n",
    "containsDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35aafe9-2a3d-43b8-8078-9f79dadc0a74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
