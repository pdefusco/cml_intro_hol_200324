{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0942afe9-7629-4b3e-b747-b795a6454a2b",
   "metadata": {},
   "source": [
    "## Introduction to Iceberg Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd15c6f-e31d-440c-bc48-966385b7c5ec",
   "metadata": {},
   "source": [
    "#### Launching a Spark Session with Iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f139b6a-eed6-4848-9e82-593aabad58b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cml.data_v1 as cmldata\n",
    "\n",
    "CONNECTION_NAME = \"go01-aw-dl\"\n",
    "conn = cmldata.get_connection(CONNECTION_NAME)\n",
    "spark = conn.get_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3bbf1bf-a0a1-4864-8c98-88dfb994060a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/16 00:58:56 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "24/03/16 00:58:56 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spark.kubernetes.driver.pod.name', 'ikgvjqk3qptnit2c'),\n",
       " ('spark.eventLog.enabled', 'true'),\n",
       " ('spark.ui.proxyRedirectUri',\n",
       "  'https://spark-ikgvjqk3qptnit2c.ml-d7f9c760-9de.go01-dem.ylcu-atmi.cloudera.site'),\n",
       " ('spark.network.crypto.enabled', 'true'),\n",
       " ('spark.sql.hive.hwc.execution.mode', 'spark'),\n",
       " ('spark.driver.port', '34509'),\n",
       " ('spark.jars',\n",
       "  '/opt/spark/optional-lib/hive-warehouse-connector-assembly.jar,/opt/spark/optional-lib/iceberg-hive-runtime.jar,/opt/spark/optional-lib/iceberg-spark-runtime.jar'),\n",
       " ('spark.driver.host', '100.100.8.239'),\n",
       " ('spark.kerberos.renewal.credentials', 'ccache'),\n",
       " ('spark.sql.catalog.spark_catalog',\n",
       "  'org.apache.iceberg.spark.SparkSessionCatalog'),\n",
       " ('spark.driver.bindAddress', '100.100.8.239'),\n",
       " ('spark.eventLog.dir', 'file:///sparkeventlogs'),\n",
       " ('spark.dynamicAllocation.maxExecutors', '249'),\n",
       " ('spark.hadoop.yarn.resourcemanager.principal', 'pauldefusco'),\n",
       " ('spark.kubernetes.driver.annotation.cluster-autoscaler.kubernetes.io/safe-to-evict',\n",
       "  'false'),\n",
       " ('spark.ui.allowFramingFrom',\n",
       "  'https://ml-d7f9c760-9de.go01-dem.ylcu-atmi.cloudera.site'),\n",
       " ('spark.ui.port', '20049'),\n",
       " ('spark.yarn.access.hadoopFileSystems',\n",
       "  's3a://go01-demo/warehouse/tablespace/external/hive'),\n",
       " ('spark.sql.extensions',\n",
       "  'com.qubole.spark.hiveacid.HiveAcidAutoConvertExtension,org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions'),\n",
       " ('spark.kubernetes.executor.annotation.cluster-autoscaler.kubernetes.io/safe-to-evict',\n",
       "  'false'),\n",
       " ('spark.io.encryption.enabled', 'true'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.app.startTime', '1710550726202'),\n",
       " ('spark.hadoop.mapreduce.outputcommitter.factory.scheme.s3a',\n",
       "  'org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.master', 'k8s://https://172.20.0.1:443'),\n",
       " ('spark.kubernetes.executor.podTemplateFile', '/tmp/spark-executor.json'),\n",
       " ('spark.dynamicAllocation.shuffleTracking.enabled', 'true'),\n",
       " ('spark.sql.sources.commitProtocolClass',\n",
       "  'org.apache.spark.internal.io.cloud.PathOutputCommitProtocol'),\n",
       " ('spark.driver.memory', '6605m'),\n",
       " ('spark.kubernetes.container.image',\n",
       "  'docker.repository.cloudera.com/cloudera/cdsw/ml-runtime-jupyterlab-python3.9-standard:2023.12.1-b8'),\n",
       " ('spark.kubernetes.executor.podNamePrefix', 'cdsw-ikgvjqk3qptnit2c'),\n",
       " ('spark.kubernetes.executor.config.dir', '/var/spark/conf'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///runtime-addons/spark323-19-hf2-e51pfs/opt/spark/optional-lib/hive-warehouse-connector-assembly-1.0.0.1.19.7215.0-118.jar,file:///runtime-addons/spark323-19-hf2-e51pfs/opt/spark/optional-lib/iceberg-hive-runtime-1.1.0.1.19.7215.0-118.jar,file:///runtime-addons/spark323-19-hf2-e51pfs/opt/spark/optional-lib/iceberg-spark-runtime-3.2_2.12-1.1.0.1.19.7215.0-118.jar'),\n",
       " ('spark.kubernetes.namespace', 'mlx-user-2'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  's3a://go01-demo/warehouse/tablespace/external/hive'),\n",
       " ('spark.hadoop.iceberg.engine.hive.enabled', 'true'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.id', 'spark-application-1710550728037'),\n",
       " ('spark.sql.parquet.output.committer.class',\n",
       "  'org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter'),\n",
       " ('spark.app.initial.jar.urls',\n",
       "  'spark://100.100.8.239:34509/jars/hive-warehouse-connector-assembly-1.0.0.1.19.7215.0-118.jar,spark://100.100.8.239:34509/jars/iceberg-spark-runtime-3.2_2.12-1.1.0.1.19.7215.0-118.jar,spark://100.100.8.239:34509/jars/iceberg-hive-runtime-1.1.0.1.19.7215.0-118.jar'),\n",
       " ('spark.yarn.rmProxy.enabled', 'false'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.kryo.registrator',\n",
       "  'com.qubole.spark.hiveacid.util.HiveAcidKyroRegistrator'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.dynamicAllocation.enabled', 'true'),\n",
       " ('spark.deploy.mode', 'client'),\n",
       " ('spark.hadoop.fs.s3a.committer.name', 'magic'),\n",
       " ('spark.app.name', 'None'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.sql.catalog.spark_catalog.type', 'hive'),\n",
       " ('spark.authenticate', 'true'),\n",
       " ('spark.hadoop.fs.s3a.committer.magic.enabled', 'true')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa007770-01e4-4c2e-a784-6955c448c952",
   "metadata": {},
   "source": [
    "### Iceberg Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad5615b-c7e6-4528-bd48-db0841d38c39",
   "metadata": {},
   "source": [
    "![alt text](img/iceberg-metadata.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c3c93c-4065-422b-9697-609ae50687c7",
   "metadata": {},
   "source": [
    "#### Iceberg Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30001dd5-c3ef-4223-84ca-e63552c4f879",
   "metadata": {},
   "source": [
    "Iceberg comes with catalogs that enable SQL commands to manage tables and load them by name. Catalogs are configured using properties under spark.sql.catalog.(catalog_name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3819a08-8e00-4184-bdc6-79b3d8507d83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/16 01:00:38 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|      catalog|namespace|\n",
      "+-------------+---------+\n",
      "|spark_catalog|  default|\n",
      "+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show catalog and database\n",
    "spark.sql(\"SHOW CURRENT NAMESPACE\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1119faa-93a7-4ebe-85b1-3ec87eb54b7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new database\n",
    "#spark.sql(\"DROP DATABASE IF EXISTS spark_catalog.lakehouse\")\n",
    "import os \n",
    "\n",
    "username = os.environ[\"PROJECT_OWNER\"]\n",
    "\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS spark_catalog.{}_lakehouse\".format(username))\n",
    "spark.sql(\"USE spark_catalog.{}_lakehouse\".format(username))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da3225e2-a19a-471c-aaed-8f4b8f833b8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"USE spark_catalog.{}_lakehouse\".format(username))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "834ca50c-9c2f-43ac-9ec0-9cf28e8507d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|      catalog|           namespace|\n",
      "+-------------+--------------------+\n",
      "|spark_catalog|pauldefusco_lakeh...|\n",
      "+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show catalog and database\n",
    "spark.sql(\"SHOW CURRENT NAMESPACE\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6cecda-848a-4cbe-b7a8-d73096cab11f",
   "metadata": {},
   "source": [
    "#### Create an Iceberg Table with Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c5daa62-cde3-440a-ad91-0c6f9b41ef3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/16 01:02:28 WARN HiveMetaStoreClient: Failed to connect to the MetaStore Server...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS {}_lakehouse.coffees_table PURGE\".format(username))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b10e6c-efcc-46b5-a164-893879b79327",
   "metadata": {},
   "source": [
    "#### Iceberg Create Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b19e0de9-29c4-43dd-ae5c-c2bd69558032",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE IF NOT EXISTS {}_lakehouse.coffees_table (coffee_id BIGINT, coffee_size STRING, coffee_sale_ts TIMESTAMP)\\\n",
    "          USING ICEBERG\\\n",
    "          PARTITIONED BY (months(coffee_sale_ts))\\\n",
    "          TBLPROPERTIES ('write.delete.mode'='copy-on-write',\\\n",
    "                          'write.update.mode'='copy-on-write',\\\n",
    "                          'write.merge.mode'='copy-on-write',\\\n",
    "                          'format-version' = '2')\".format(username))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eea95e7-e221-4997-9071-208cbc9f9fc5",
   "metadata": {},
   "source": [
    "#### Verify that a Metadata JSON file has been created under the Metadata directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f6b96e-d878-4e2e-91a0-1ef785833479",
   "metadata": {},
   "source": [
    "#### Notice that no snapshots or other files have been created as data has not yet been inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "972dced2-83a3-4c4f-a63d-2936a54534d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+---------+-------------------+\n",
      "|made_current_at|snapshot_id|parent_id|is_current_ancestor|\n",
      "+---------------+-----------+---------+-------------------+\n",
      "+---------------+-----------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM {}_lakehouse.coffees_table.history\".format(username)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d726f825-38b0-4342-a0ea-0f932ce20355",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+---------+---------+-------------+-------+\n",
      "|committed_at|snapshot_id|parent_id|operation|manifest_list|summary|\n",
      "+------------+-----------+---------+---------+-------------+-------+\n",
      "+------------+-----------+---------+---------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM {}_lakehouse.coffees_table.snapshots\".format(username)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68f5ad55-0adb-4ab3-8574-da44c3af6b3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------+-------+---------+------------+------------------+------------+------------+-----------------+----------------+------------+------------+------------+-------------+------------+-------------+\n",
      "|content|file_path|file_format|spec_id|partition|record_count|file_size_in_bytes|column_sizes|value_counts|null_value_counts|nan_value_counts|lower_bounds|upper_bounds|key_metadata|split_offsets|equality_ids|sort_order_id|\n",
      "+-------+---------+-----------+-------+---------+------------+------------------+------------+------------+-----------------+----------------+------------+------------+------------+-------------+------------+-------------+\n",
      "+-------+---------+-----------+-------+---------+------------+------------------+------------+------------+-----------------+----------------+------------+------------+------------+-------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM {}_lakehouse.coffees_table.files\".format(username)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "636820a4-528b-42f2-9153-2180206420de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+-----------------+-----------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+-------------------+\n",
      "|content|path|length|partition_spec_id|added_snapshot_id|added_data_files_count|existing_data_files_count|deleted_data_files_count|added_delete_files_count|existing_delete_files_count|deleted_delete_files_count|partition_summaries|\n",
      "+-------+----+------+-----------------+-----------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+-------------------+\n",
      "+-------+----+------+-----------------+-----------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM {}_lakehouse.coffees_table.manifests\".format(username)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8544499-8ba1-4156-b641-89c10c281926",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------+-------+---------+------------+------------------+------------+------------+-----------------+----------------+------------+------------+------------+-------------+------------+-------------+\n",
      "|content|file_path|file_format|spec_id|partition|record_count|file_size_in_bytes|column_sizes|value_counts|null_value_counts|nan_value_counts|lower_bounds|upper_bounds|key_metadata|split_offsets|equality_ids|sort_order_id|\n",
      "+-------+---------+-----------+-------+---------+------------+------------------+------------+------------+-----------------+----------------+------------+------------+------------+-------------+------------+-------------+\n",
      "+-------+---------+-----------+-------+---------+------------+------------------+------------+------------+-----------------+----------------+------------+------------+------------+-------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM {}_lakehouse.coffees_table.all_data_files\".format(username)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "323639fc-1ed3-418d-ae0e-8c1674a30ac9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+-----------------+-----------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+-------------------+---------------------+\n",
      "|content|path|length|partition_spec_id|added_snapshot_id|added_data_files_count|existing_data_files_count|deleted_data_files_count|added_delete_files_count|existing_delete_files_count|deleted_delete_files_count|partition_summaries|reference_snapshot_id|\n",
      "+-------+----+------+-----------------+-----------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+-------------------+---------------------+\n",
      "+-------+----+------+-----------------+-----------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+-------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM {}_lakehouse.coffees_table.all_manifests\".format(username)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24046132-45da-4f28-a60b-f0227b8fb7cc",
   "metadata": {},
   "source": [
    "### Table Insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "028f245c-e167-4418-9e96-fd098e0cb6cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f2d7725-fe94-4a8c-82d0-319c1cefef8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Coffee_id = 1, Coffee_size = venti, coffee_sale_ts = 2023-09-01\n",
    "\n",
    "spark.sql(\"INSERT INTO {}_lakehouse.coffees_table VALUES (1, 'venti', cast(date_format('2023-09-01 10:00:00', 'yyyy-MM-dd HH:mm:ss') as timestamp))\".format(username))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f0d668-9e68-4b1c-a881-5d1265f999c7",
   "metadata": {},
   "source": [
    "#### Data has been added to the data folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3728c7dc-ac75-4859-8790-501210b43381",
   "metadata": {},
   "source": [
    "###### Notice that the history and snapshots tables have now been populated with a record reflecting the first INSERT operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da0242b8-93b9-4471-9aae-7a8f162a2320",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "QUERY = \"select h.made_current_at,\\\n",
    "            s.operation,\\\n",
    "            h.snapshot_id,\\\n",
    "            h.is_current_ancestor,\\\n",
    "            s.summary['spark.app.id']\\\n",
    "        from {0}_lakehouse.coffees_table.history h\\\n",
    "        join {0}_lakehouse.coffees_table.snapshots s\\\n",
    "            on h.snapshot_id = s.snapshot_id\\\n",
    "            order by made_current_at;\".format(username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76eaa58b-c77b-437a-979d-feaa4ac9b536",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/16 01:09:59 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------------------+-------------------+---------------------+\n",
      "|     made_current_at|operation|        snapshot_id|is_current_ancestor|summary[spark.app.id]|\n",
      "+--------------------+---------+-------------------+-------------------+---------------------+\n",
      "|2024-03-16 01:09:...|   append|4358765587127049418|               true| spark-application...|\n",
      "+--------------------+---------+-------------------+-------------------+---------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(QUERY).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb91747c-9f0b-46cc-b363-fc134d97a9e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+-----------------+-------------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+--------------------+---------------------+\n",
      "|content|                path|length|partition_spec_id|  added_snapshot_id|added_data_files_count|existing_data_files_count|deleted_data_files_count|added_delete_files_count|existing_delete_files_count|deleted_delete_files_count| partition_summaries|reference_snapshot_id|\n",
      "+-------+--------------------+------+-----------------+-------------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+--------------------+---------------------+\n",
      "|      0|s3a://go01-demo/w...|  7076|                0|4358765587127049418|                     1|                        0|                       0|                       0|                          0|                         0|[{false, false, 2...|  4358765587127049418|\n",
      "+-------+--------------------+------+-----------------+-------------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM {}_lakehouse.coffees_table.all_manifests\".format(username)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f023c1ca-48c8-4d4d-b329-4825d473bf4b",
   "metadata": {},
   "source": [
    "### Table Merge Into"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bef6e0-6ea1-4f75-aeac-fd84b0b7f2f7",
   "metadata": {},
   "source": [
    "Create a staging table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8287cfa-9f8e-40fb-a880-a2dabcdbd2ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS {}_lakehouse.coffee_staging PURGE\".format(username))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a742c99-a211-4fa6-bdd5-e63f8079f9d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE IF NOT EXISTS {}_lakehouse.coffee_staging\\\n",
    "            (coffee_id BIGINT, coffee_size STRING, coffee_sale_ts TIMESTAMP)\\\n",
    "            USING iceberg\\\n",
    "            PARTITIONED BY (months(coffee_sale_ts))\".format(username))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4dc4207-34e0-4bb3-a147-dfcd4e601990",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"INSERT INTO {}_lakehouse.coffee_staging\\\n",
    "            VALUES (1, 'grande', cast(date_format('2023-07-01 11:00:00', 'yyyy-MM-dd HH:mm:ss') as timestamp)),\\\n",
    "            (2, 'grande', cast(date_format('2023-07-01 11:10:00', 'yyyy-MM-dd HH:mm:ss') as timestamp)),\\\n",
    "            (3, 'tall', cast(date_format('2023-04-01 12:01:00', 'yyyy-MM-dd HH:mm:ss') as timestamp))\".format(username))\n",
    "\n",
    "#Row: Coffee_id = 1, coffee_size = venti, coffee_sale_ts = 2023-07-01\n",
    "#Row: Coffee_id = 2, coffee_size = grande, coffee_sale_ts = 2023-07-01\n",
    "#Row: Coffee_id = 3, coffee_size = tall, coffee_sale_ts = 2023-04-01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c3720c-3011-4a15-924f-de39a525501d",
   "metadata": {},
   "source": [
    "Merge Into Customers Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b62e634-935e-4219-b240-fdb8411b2984",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/16 01:14:25 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"MERGE INTO {0}_lakehouse.coffees_table c\\\n",
    "            USING (SELECT * FROM {0}_lakehouse.coffee_staging) s\\\n",
    "            ON c.coffee_id = s.coffee_id \\\n",
    "            WHEN MATCHED THEN UPDATE SET c.coffee_size = s.coffee_size\\\n",
    "            WHEN NOT MATCHED THEN INSERT *\".format(username))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c94305e6-c808-4d67-8903-864a4d80b5dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|        committed_at|        snapshot_id|          parent_id|operation|       manifest_list|             summary|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|2024-03-16 01:09:...|4358765587127049418|               null|   append|s3a://go01-demo/w...|{spark.app.id -> ...|\n",
      "|2024-03-16 01:14:...|5428666139761163381|4358765587127049418|overwrite|s3a://go01-demo/w...|{spark.app.id -> ...|\n",
      "|2024-03-16 01:14:...| 779761274742802786|5428666139761163381|overwrite|s3a://go01-demo/w...|{spark.app.id -> ...|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM {}_lakehouse.coffees_table.snapshots;\".format(username)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d2d2db3-6d38-428e-bc8d-51286ff2e11c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>path</th>\n",
       "      <th>length</th>\n",
       "      <th>partition_spec_id</th>\n",
       "      <th>added_snapshot_id</th>\n",
       "      <th>added_data_files_count</th>\n",
       "      <th>existing_data_files_count</th>\n",
       "      <th>deleted_data_files_count</th>\n",
       "      <th>added_delete_files_count</th>\n",
       "      <th>existing_delete_files_count</th>\n",
       "      <th>deleted_delete_files_count</th>\n",
       "      <th>partition_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>7216</td>\n",
       "      <td>0</td>\n",
       "      <td>779761274742802786</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(False, False, 2023-04, 2023-09)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>7214</td>\n",
       "      <td>0</td>\n",
       "      <td>779761274742802786</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(False, False, 2023-04, 2023-09)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   content                                               path  length  \\\n",
       "0        0  s3a://go01-demo/warehouse/tablespace/external/...    7216   \n",
       "1        0  s3a://go01-demo/warehouse/tablespace/external/...    7214   \n",
       "\n",
       "   partition_spec_id   added_snapshot_id  added_data_files_count  \\\n",
       "0                  0  779761274742802786                       3   \n",
       "1                  0  779761274742802786                       0   \n",
       "\n",
       "   existing_data_files_count  deleted_data_files_count  \\\n",
       "0                          0                         0   \n",
       "1                          0                         3   \n",
       "\n",
       "   added_delete_files_count  existing_delete_files_count  \\\n",
       "0                         0                            0   \n",
       "1                         0                            0   \n",
       "\n",
       "   deleted_delete_files_count                 partition_summaries  \n",
       "0                           0  [(False, False, 2023-04, 2023-09)]  \n",
       "1                           0  [(False, False, 2023-04, 2023-09)]  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM {}_lakehouse.coffees_table.manifests;\".format(username)).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "db59e20d-b4fa-455c-8a34-b5538d06b2ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>file_path</th>\n",
       "      <th>file_format</th>\n",
       "      <th>spec_id</th>\n",
       "      <th>partition</th>\n",
       "      <th>record_count</th>\n",
       "      <th>file_size_in_bytes</th>\n",
       "      <th>column_sizes</th>\n",
       "      <th>value_counts</th>\n",
       "      <th>null_value_counts</th>\n",
       "      <th>nan_value_counts</th>\n",
       "      <th>lower_bounds</th>\n",
       "      <th>upper_bounds</th>\n",
       "      <th>key_metadata</th>\n",
       "      <th>split_offsets</th>\n",
       "      <th>equality_ids</th>\n",
       "      <th>sort_order_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>PARQUET</td>\n",
       "      <td>0</td>\n",
       "      <td>(644,)</td>\n",
       "      <td>1</td>\n",
       "      <td>976</td>\n",
       "      <td>{1: 33, 2: 34, 3: 39}</td>\n",
       "      <td>{1: 1, 2: 1, 3: 1}</td>\n",
       "      <td>{1: 0, 2: 0, 3: 0}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [118, 101, 11...</td>\n",
       "      <td>{1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [118, 101, 11...</td>\n",
       "      <td>None</td>\n",
       "      <td>[4]</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>PARQUET</td>\n",
       "      <td>0</td>\n",
       "      <td>(642,)</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>{1: 39, 2: 41, 3: 39}</td>\n",
       "      <td>{1: 1, 2: 1, 3: 1}</td>\n",
       "      <td>{1: 0, 2: 0, 3: 0}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{1: [2, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...</td>\n",
       "      <td>{1: [2, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...</td>\n",
       "      <td>None</td>\n",
       "      <td>[4]</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>PARQUET</td>\n",
       "      <td>0</td>\n",
       "      <td>(644,)</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>{1: 39, 2: 41, 3: 39}</td>\n",
       "      <td>{1: 1, 2: 1, 3: 1}</td>\n",
       "      <td>{1: 0, 2: 0, 3: 0}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...</td>\n",
       "      <td>{1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...</td>\n",
       "      <td>None</td>\n",
       "      <td>[4]</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>PARQUET</td>\n",
       "      <td>0</td>\n",
       "      <td>(639,)</td>\n",
       "      <td>1</td>\n",
       "      <td>985</td>\n",
       "      <td>{1: 39, 2: 39, 3: 39}</td>\n",
       "      <td>{1: 1, 2: 1, 3: 1}</td>\n",
       "      <td>{1: 0, 2: 0, 3: 0}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{1: [3, 0, 0, 0, 0, 0, 0, 0], 2: [116, 97, 108...</td>\n",
       "      <td>{1: [3, 0, 0, 0, 0, 0, 0, 0], 2: [116, 97, 108...</td>\n",
       "      <td>None</td>\n",
       "      <td>[4]</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>PARQUET</td>\n",
       "      <td>0</td>\n",
       "      <td>(642,)</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>{1: 39, 2: 41, 3: 39}</td>\n",
       "      <td>{1: 1, 2: 1, 3: 1}</td>\n",
       "      <td>{1: 0, 2: 0, 3: 0}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{1: [2, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...</td>\n",
       "      <td>{1: [2, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...</td>\n",
       "      <td>None</td>\n",
       "      <td>[4]</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>PARQUET</td>\n",
       "      <td>0</td>\n",
       "      <td>(644,)</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>{1: 39, 2: 41, 3: 39}</td>\n",
       "      <td>{1: 1, 2: 1, 3: 1}</td>\n",
       "      <td>{1: 0, 2: 0, 3: 0}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...</td>\n",
       "      <td>{1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...</td>\n",
       "      <td>None</td>\n",
       "      <td>[4]</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>s3a://go01-demo/warehouse/tablespace/external/...</td>\n",
       "      <td>PARQUET</td>\n",
       "      <td>0</td>\n",
       "      <td>(639,)</td>\n",
       "      <td>1</td>\n",
       "      <td>985</td>\n",
       "      <td>{1: 39, 2: 39, 3: 39}</td>\n",
       "      <td>{1: 1, 2: 1, 3: 1}</td>\n",
       "      <td>{1: 0, 2: 0, 3: 0}</td>\n",
       "      <td>{}</td>\n",
       "      <td>{1: [3, 0, 0, 0, 0, 0, 0, 0], 2: [116, 97, 108...</td>\n",
       "      <td>{1: [3, 0, 0, 0, 0, 0, 0, 0], 2: [116, 97, 108...</td>\n",
       "      <td>None</td>\n",
       "      <td>[4]</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   content                                          file_path file_format  \\\n",
       "0        0  s3a://go01-demo/warehouse/tablespace/external/...     PARQUET   \n",
       "1        0  s3a://go01-demo/warehouse/tablespace/external/...     PARQUET   \n",
       "2        0  s3a://go01-demo/warehouse/tablespace/external/...     PARQUET   \n",
       "3        0  s3a://go01-demo/warehouse/tablespace/external/...     PARQUET   \n",
       "4        0  s3a://go01-demo/warehouse/tablespace/external/...     PARQUET   \n",
       "5        0  s3a://go01-demo/warehouse/tablespace/external/...     PARQUET   \n",
       "6        0  s3a://go01-demo/warehouse/tablespace/external/...     PARQUET   \n",
       "\n",
       "   spec_id partition  record_count  file_size_in_bytes           column_sizes  \\\n",
       "0        0    (644,)             1                 976  {1: 33, 2: 34, 3: 39}   \n",
       "1        0    (642,)             1                 999  {1: 39, 2: 41, 3: 39}   \n",
       "2        0    (644,)             1                 999  {1: 39, 2: 41, 3: 39}   \n",
       "3        0    (639,)             1                 985  {1: 39, 2: 39, 3: 39}   \n",
       "4        0    (642,)             1                 999  {1: 39, 2: 41, 3: 39}   \n",
       "5        0    (644,)             1                 999  {1: 39, 2: 41, 3: 39}   \n",
       "6        0    (639,)             1                 985  {1: 39, 2: 39, 3: 39}   \n",
       "\n",
       "         value_counts   null_value_counts nan_value_counts  \\\n",
       "0  {1: 1, 2: 1, 3: 1}  {1: 0, 2: 0, 3: 0}               {}   \n",
       "1  {1: 1, 2: 1, 3: 1}  {1: 0, 2: 0, 3: 0}               {}   \n",
       "2  {1: 1, 2: 1, 3: 1}  {1: 0, 2: 0, 3: 0}               {}   \n",
       "3  {1: 1, 2: 1, 3: 1}  {1: 0, 2: 0, 3: 0}               {}   \n",
       "4  {1: 1, 2: 1, 3: 1}  {1: 0, 2: 0, 3: 0}               {}   \n",
       "5  {1: 1, 2: 1, 3: 1}  {1: 0, 2: 0, 3: 0}               {}   \n",
       "6  {1: 1, 2: 1, 3: 1}  {1: 0, 2: 0, 3: 0}               {}   \n",
       "\n",
       "                                        lower_bounds  \\\n",
       "0  {1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [118, 101, 11...   \n",
       "1  {1: [2, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...   \n",
       "2  {1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...   \n",
       "3  {1: [3, 0, 0, 0, 0, 0, 0, 0], 2: [116, 97, 108...   \n",
       "4  {1: [2, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...   \n",
       "5  {1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...   \n",
       "6  {1: [3, 0, 0, 0, 0, 0, 0, 0], 2: [116, 97, 108...   \n",
       "\n",
       "                                        upper_bounds key_metadata  \\\n",
       "0  {1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [118, 101, 11...         None   \n",
       "1  {1: [2, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...         None   \n",
       "2  {1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...         None   \n",
       "3  {1: [3, 0, 0, 0, 0, 0, 0, 0], 2: [116, 97, 108...         None   \n",
       "4  {1: [2, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...         None   \n",
       "5  {1: [1, 0, 0, 0, 0, 0, 0, 0], 2: [103, 114, 97...         None   \n",
       "6  {1: [3, 0, 0, 0, 0, 0, 0, 0], 2: [116, 97, 108...         None   \n",
       "\n",
       "  split_offsets equality_ids  sort_order_id  \n",
       "0           [4]         None              0  \n",
       "1           [4]         None              0  \n",
       "2           [4]         None              0  \n",
       "3           [4]         None              0  \n",
       "4           [4]         None              0  \n",
       "5           [4]         None              0  \n",
       "6           [4]         None              0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM {}_lakehouse.coffees_table.all_data_files;\".format(username)).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67491c12-86aa-4ced-acf4-8926bfedd947",
   "metadata": {},
   "source": [
    "RECAP: \n",
    "1. The target table coffee was empty. \n",
    "2. We ran an insert and that created one new metadata file, one new manifest file, and one new data file.\n",
    "3. Then we ran a Merge Into from the staging table. Of the three rows that we compared against, one was a match and two were not.\n",
    "    The one that was a match was rewritten to a brand new data file in the same partition (coffee_sale_ts_month=2023-09) which now has two data files.\n",
    "    The remaining two rows that were not a match were written to two new data files in the two respective partitions. If they had fallen under the same partition they may have been written into the same data file (next example)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
